%!TEX root = ../main.tex

\chapter{绪论}

\section{课题来源及研究的背景和意义}
\subsection{课题研究的背景}
目前，随着Web2.0时代的到来，社交网络拥有十分庞大的用户群体，随着互联网的发展，在国外有Facebook、Twitter等社交网络，在国内也兴起了诸如新浪微博、腾讯微博、人人网、微信等社交网络，在2016年12月为止，仅仅中国的社交网络用户就达到7.31亿人\citeup{CNNIC39}。这些社交网站拥有庞大的用户量，每个用户每天制造着大量不同的数据。通过研究社交网络中的信息传播规律，有助于我们对复杂网络的传播能力有进一步认识。同时，利用信息传播的规律对社交网络上信息的整体传播趋势进行预测，也是一个值得研究的问题。
\subsection{课题研究的目的和意义}
社交网络和各新闻媒体中每天发布着大量的信息，吸引用户进行转发、评论等操作，这些数据为研究人类行为和复杂社区提供了丰富的真实数据。借用这些数据进行数据挖掘，有助于了解每个用户的关注点和行为规律，有利于提早发现火热内容，进而为社交网站的商业模式提供更加了准确的决策服务。

各大企业经常在社交网络、视频、新闻媒体等互联网中投放广告，以吸引用户关注；同时有针对性的进行内容推送有助于提升用户体验，给企业带来盈利点；而对热门内容分配更大的带宽，有利于完善系统性能。对于企业来说，如何在最少的投入下，获得最大效益的经济回报是它们关注的重点，因此该研究有实用的商业价值。

同时，对政府来说，了解网民关注热点，把握社会脉搏，预知舆论动态，对于政府决策的定制和调整也起着重要的作用。互联网中信息交互快速，有很多人以个人或团体形式在各新媒体平台中散播虚假、错误、危害安全的信息。掌握信息传播的机制，预测内容热度，对热门内容进行预警，有助于政府对突发事件提供辅助的决策支持，有助于提早进行舆情调控。

在统计多个数据集中各类别的内容传播情况时，我们注意到类别与类别之间存在相互作用关系，尤其是当一个类别特别受到关注的时候，另一个类别的关注度就会有所减少。同时，用户行为也可以反映出类别相互作用关系，当大部分用户反常地去关注平时不吸引该用户的类别的时候，说明当前这个类别抢占其他类别的关注度。因此，在热度预测任务中加入类别与类别之间的相互作用关系，以及用户和类别的相互作用关系能够提升预测效果。

本文中，我们将事件与用户按偏好属性分类，并将从两个方面来计算这些参量之间的相互作用关系衡量类别的相互作用关系。(1)利用信息与当前同时在网络中传播的信息之间的影响来量化类别相互作用关系。(2)利用当前网络中活跃用户作为探针节点，计算活跃用户的偏好，通过活跃用户是否受信息感染来衡量类别之间的相互作用关系。


\section{相关工作}
在线社交网络的信息传播问题已经成为研究的热点，也取得了很多重要的成果\citeup{bakshy2011everyone}\citeup{leskovec2007patterns}。对于多元信息传播分析中关于竞争扩散传播模型的研究还处于初级阶段，相关研究工作不多。针对社交网络信息传播的研究，本文将从两个方面进行介绍。

\subsection{信息传播模型}
在过去几年中，在线社交网络中信息传播的研究主要集中在单条信息在社交网络中的传播。基于网络结构的传播模型主要由Granovetter\citeup{granovetter1978threshold}\citeup{schelling1978micromotives}提出的线性阈值模型，模型把每个节点分为激活态和非激活态，每个节点一旦在某次迭代$t$中变为激活态，它将保持直到迭代结束。并且，该节点将在今后的$t+1,t+2,...$次迭代中，不断激活它的邻居节点。而阈值指的是在迭代开始的时候，每个节点被随机分配一个值$\theta$，$\theta$越大越容易被激活。Goldenberg\citeup{mikolov2013distributed}等人提出的独立级联模型，该模型中把每个用户当成有向图或无向图中的一个节点，他们会产生自己的扩散级联，并且每个级联之间是相互独立的，互不干扰。模型假设每个节点感染与他相连的其他节点的概率为$p\in[0,1]$，与线性阈值模型不同的是，每个节点被激活后仅有一次机会激活其邻居节点，迭代计算直到没有节点再被感染为止。社交网络信息传播中很多模型都是基于线性阈值模型和独立级联模型进行扩展。比如Kempe\citeup{kempe2003maximizing}提出的WC模型用于解决影响力最大化问题，实际上该问题被证明为NP-难问题。该模型也是通过给边分配权重来判断是否会被邻居节点感染，每条边的权重主要由节点度以及网络结构来决定，他们把线性阈值模型和独立级联模型同贪心算法结合起来，从而NP-难问题得到解决。但是实际上，各种基于线性阈值模型和独立级联模型的拓展模型有个共同的缺点，他们模拟的传播过程解释过于简单随意，并非真实的概率传播。

基于群体状态的传播模型主要由Girvan\citeup{girvan2002community}提出的SIR模型，是在传统的传染病模型以及SIS模型\citeup{hethcote2000mathematics}\citeup{may2001infection}的基础上加入免疫和突变等信息来解释传染病的演化，SIS模型假设网络中的每个节点有易受感染态和已受感染态两种状态，在模型中假设每个节点被其他节点感染的概率为 ，与独立级联模型不同的是， 是一个只与传播信息内容有关的全局概率，与感染和传播的用户无关。与SIS不同的是，SIR模型中，每个节点除了SIS模型的易受感染态和已受感染态之外，还有一个免疫状态，表示一个节点被治愈后不能再被其他节点感染。Yang\citeup{yang2010modeling}等人提出线性影响力模型假设信息传播过程中由个别影响力节点掌控，通过信息传播中通过的节点影响力来计算传播趋势，在信息传播的过程中只考虑了时间因素，忽略了信息在传播过程中的空间因素，即用户间之间的相互关系。


\subsubsection{基于包装器的内容抽取算法}

最简单和直接的Web内容抽取方法是手工构建包装器（Wrapper），
包装器是信息集成系统的一个独立模块，抽取网页数据并将其转换为结构化的数据。
但这个过程容易出错也很耗费人力资源，随后一些研究致力于简化包装器构造的过程，降低专业知识门槛，
例如XWrap\citeup{liu2000xwrap}和W4F\citeup{sahuguet2001building}。

XWrap（XML-enabled Wrapper）
通过和用户的交互，推导出信息抽取规则，交互式生成XML形式的包装器。
W4F（World Wide Web Wrapper Factory）
提供了一个更有表现力的领域专用语言描述复杂的抽取规则，并利用图形化工具辅助构造包装器。
这些包装器使用的技术包括正则表达式、XPath、甚至特定领域的编程语言，
用来抽取嵌入在半结构化HTML网页中的文本。

虽然这些手工构造的包装器针对特定站点能够取得很高的抽取准确率，但它们的缺点也很明显，
需要大量的人力成本，对大规模的内容抽取任务来说手工构造包装器是难以接受的。

\subsubsection{基于机器学习的内容抽取算法}

基于机器学习的方法主要利用网页的结构、语言学等特征，在人工标注的数据集上进行训练，
根据训练出的分类模型，区分网页中的主要内容和噪声数据。

Kushmerick\citeup{kushmerick1999learning}和Davison\citeup{davison2000recognizing}
提出使用机器学习的方法来识别网页中的广告、冗余和不相关的链接。
Marek等人\citeup{marek2007web}提出了一种利用
条件随机场（Conditional Random Fields，CRF）的方法来识别网页中的核心内容，
首先通过HTML标签，将训练文件分成若干块，对于每一块提取markup-based、content-based和
document-related的特征。依据这些特征，将内容块标记为若干类，训练条件随机场模型进行分类。
FIASCO\citeup{bauer2007fiasco}基于
支持向量机（Support Vector Machine，SVM）来识别网页中的噪声数据。
首先将网页文件解析成DOM树，对每一个目标节点提取语言学特征、结构特征和视觉特征，
人工选择目标节点标注为clean或dirty，然后使用SVM训练分类器。

然而这些方法难以推广使用，因为它们依赖大量人工标记的训练数据集和领域专业知识来生成分类规则。

\subsubsection{基于视觉信息的内容抽取算法}

2003年微软亚洲研究院的蔡登提出了一种基于视觉信息的抽取算法
VIPS（VIsion based Page Segmentation）\citeup{cai2003vips}。
基于前人对于用户视觉心理的研究，一般情况下网页的核心内容处于网页的中间位置，
而导航栏、广告和推广链接则处于网页的周边位置。
这种方法利用网页的视觉信息，根据一些启发式规则，首先将网页分块，对不同的分块赋予相应的权重，
然后进行删除、合并操作，最终确定网页的核心内容块。

随后宋瑞华\citeup{song2004learning}在VIPS的基础上，使用人工标注的训练集，
根据不同分块的位置特征和内容特征，用SVM和神经网络训练分类模型。
Fernandes\citeup{fernandes2007computing}首先使用VIPS网页分块算法，
将每个网页分成若干个由DOM树中的路径及其文本内容定义的块，
在上述基础上使用向量空间模型（Vector Space Model，VSM）思想，计算其重要程度。

基于视觉信息分块的方法具有很好的通用性，因为不同网页的源代码结构可能差别很大，
但经浏览器渲染之后的视觉表现却很接近。但这种方法必须对网页进行渲染才能利用视觉信息，
非常消耗计算资源，而且有时候渲染所需的网页样式文件不一定是可获取的。

\subsubsection{基于模板检测的内容抽取算法}

如今大部分网页都是使用固定模板动态生成的，网站后台程序从数据库中提取信息，
与模板进行渲染就形成了最终的HMTL源文件。
这样的模板在同一个网站被大量重复使用，就可以通过一组相同或相似模板生成的网页，
逆向推导出共同的模板结构，以此提取网页核心内容，这就是模板检测算法。

Lin等人\citeup{lin2002discovering}提出一种基于当时HTML文档普遍存在的制表标签的方法。
将网页划分为若干内容块，然后根据某些特征计算每个块的信息熵，
由动态调整的阈值把内容块划分为核心内容块和噪声数据。
Yi等人\citeup{yi2003eliminating}
在DOM树的基础上提出一个新型树结构，网站风格树（Site Style Tree，SST）。
由于噪声块通常有相似的内容和表现的样式，如果相同内容和样式在多个网页中重复出现，
表明其信息熵较低，可能是冗余内容。
文章提出一种基于信息熵的度量方法来确定SST的哪部分代表噪声，哪部分代表主要内容。
将网页DOM树匹配到网站的SST的过程，就能够检测并消除噪声部分，从而提取主要内容。

Bar-Yossef等人\citeup{bar2002template}
提出利用经典的频繁项挖掘来检测网页的模板，类似的研究还有文献\cite{chen2006template}。

模板检测算法在模板生成后，实际提取网页内容的计算开销很小。
但这类方法只能针对一个模板生成的网页进行处理，对于不同模板生成的网页，都需要重新构造模板。
随着Web的发展，一个网站所使用的模板越来越丰富和多样，网站还会不定期改版，
之前生成的模板就会失效，这些都给模板维护工作带来巨大负担。

\subsubsection{基于统计规律的内容抽取算法}

还有一类基于统计规律和启发式规则的内容提取算法，不需要一组网页进行训练因而与模板无关，
而且自动化程度较高。

BTE（Body Text Extraction）\citeup{finn2001fact}
将HTML网页处理为一个由词语和标签构成的序列，从中找到一个连续的区段，
使得这个区段包含最多的词语，并且包含最少的标签。
一般网页的正文包含较少的标签和相对密集的文字，这样的区段就认为是核心内容。
FE（Feature Extractor）\citeup{debnath2005automatic}
和KFE（K-Feature Extractor）\citeup{debnath2005identifying}
将网页划分为若干块，分析特定的文本、图片和标签等特征，
从中找出出现次数较少，并且最符合期望特征的块。
LQF（Link Quota Filters）\citeup{mantratzis2005separating}
利用网页分块中的链接比重来识别导航栏或相似的噪声内容。
停止词和其它一些启发式规则也在一些工作中出现\citeup{gottron2008combining}。

2008年的CCB（Content Code Blurring）\citeup{gottron2008content}
将网页HTML代码视为一个向量，生成二元的Content Code Vector（CCV）。
网页源文件中的文本内容在向量中为1，HTML代码在向量中为0，这样就把网页转换为一个二元向量。
随后对向量进行处理，从中找出同质化格式的内容，即连续为1的部分。
2010年的CETR（Content Extraction via Tag Ratios）\citeup{weninger2010cetr}
对网页源文件一行一行处理，计算出每行的Tag Ratios，即一行中的实际字符数除以HTML标签数。
Tag Ratio越高就越可能是主要内容，作者最后将计算结果映射到二维空间，利用聚类算法进行处理。
这两种算法简单、高效不需要训练数据，也不需要解析DOM树或者渲染网页，
但按行处理的方式没有利用HTML的结构化信息，对网页代码风格也较为敏感。

2011年的CETD（Content Extraction via Text Density）\citeup{sun2011dom}
首先将HTML网页解析为DOM树，然后对每个节点计算Text Density，
即该DOM节点包含的字符数除以标签数，在考虑到链接与噪声的相关性后，
提出Composite Text Density，对文本密度做了一些修正。
最后提出一种结合DOM树的DensitySum计算方式替换常用的平滑和阈值划分方法，最终确定核心内容块。

2013年的CEPR（Content Extraction via Path Ratios）\citeup{wu2013web}
认为网页内容布局与其DOM树的标签路径之间存在隐含的关联，针对每个节点计算文本标签路径比，
即文本长度与标签路径出现次数的比值。
文本标签路径比越高，表明这个路径模式承载的文本信息越多，越可能是主要内容。
在文献\cite{weninger2010cetr}的高斯平滑算法基础上，
作者提出以编辑距离作为权重，改善平滑效果，然后通过阈值方法抽取主要内容。

\subsection{Web数据记录抽取}
网页具有半结构化的特点，不同于XML或数据库记录等高度结构化的文档，不能被计算机程序直接读取。
Web数据记录抽取是从网页中抽取结构化数据，例如抽取、整合来自不同数据源的商品的各项元数据信息，
抽取论坛中帖子的发帖时间、发帖内容等元数据信息。

\subsubsection{半自动数据记录抽取算法}

人工编写包装器来提取网页信息的方法需要大量人力投入，成本高也并不实用。
于是研究人员实现了一系列工具来自动生成包装器，能够从用户的训练样本中归纳学习抽取规则，
降低了包装器构造的成本。
WIEN\citeup{kushmerick1997wrapper}，SoftMealy\citeup{hsu1998generating}
和Stalker\citeup{muslea1999hierarchical}是这类半自动化方法的典型代表。

WIEN基于多种不同的归纳学习技术，并提出了一个混合系统，训练阶段只需要少量人工参与。
SoftMealy基于有限状态转换器（Finite State Transducer，FST）和上下文规则，
通过自底向上的归纳学习方法，由人工标注的样本文件生成包装器。
它们的主要区别是抽取架构不同，WIEN使用一遍处理的LR结构，Stalker使用多遍处理的层级结构。

国内孟小峰\citeup{孟小峰2001xwis}
提出了一种基于预定义模式的方法来构造HTML包装器，
并将它运用到XWIS（基于XML的Web信息查询系统）中，
由用户定义模式并给出模式与HTML页面的映射关系，随后推导出规则构造包装器。

另一类基于模板树匹配的方法，首先从标注数据生成模板树，
模板树中映射了数据记录的相应位置，
随后对同一类型的网页，运行树匹配算法，即可抽取结构化数据。
Chuang等人\citeup{chuang2004tree}
提出一种模板树自动生成算法TTAG，能够从少量的训练网页中学习模板的树型结构，
适用于频繁更新的网站。
Zheng等人\citeup{zheng2009efficient}
提出一种新的Broom结构来同时表示数据记录和生成的模板，能够有效抽取数据记录并识别内部的语义。

国内李效东\citeup{李效东2002基于}
提出一种归纳学习算法来半自动地生成提取规则，
利用DOM树中的路径作为信息抽取的坐标，指导抽取过程。
算法是一个典型的顺序覆盖算法，产生一个假设去覆盖集合中尽可能多的正例，再删除被覆盖的正例，
循环直至所有的元素被覆盖。

这类半自动化方法需要人工标注的样本，这个过程依然费时费力，
另外模板的生成和维护都需要很大的成本，不适用于互联网规模的网络信息抽取工作。

\subsubsection{全自动数据记录抽取算法}

为了克服依赖人工标注数据的缺陷，一部分研究工作聚焦于全自动化的信息抽取方法，
这类方法更适合互联网环境下的大规模信息抽取工作。

Embley\citeup{embley1999record}
提出一种基于启发式规则的数据记录抽取方法，将网页文档的结构以嵌套标签树的形式刻画，
然后定位出包含数据记录的子树，使用五种独立的启发式规则来识别候选的记录分隔符。
Buttler提出一个全自动化的对象抽取系统Omini\citeup{buttler2001fully}，
Omini将网页解析为树型结构然后分两步抽取对象，定位子树和确定记录分隔符。
它同样采用了多种启发式规则，主要贡献是自动化的规则学习算法。
这部分方法过于依赖启发式规则，难以大规模扩展，而且如今的网页结构和布局也发生了很大变化。

Chang等人提出一个能够从网页中自动发现抽取规则的系统IEPAD\citeup{chang2001iepad}，
识别数据记录用到了重复模式的挖掘和多序列对齐（multiple sequence alignment）。
重复模式的挖掘通过一种新的数据结构PAT树来实现，并经过模式对齐进一步扩展以理解所有的记录实例。
Wang等人在此基础上提出了DeLa\citeup{wang2003data}，
该系统通过HTML表单提交查询，自动生成基于正则表达式的包装器从结果页中抽取数据对象。

上述单纯基于HTML标签序列模式特征的方法不能很好地利用网页的层次结构信息，
部分研究人员开始在DOM树上识别重复相似子树，进而抽取数据记录，
MDR\citeup{liu2003mining}是其中的典型代表，它依据这样的假设：
1）一组相似的数据记录通常位于连续的区域并具有相似的HTML标签结构；
2）每个数据记录之下包含相同数目的兄弟子树。
MDR方法简单，抽取结果也由于Omini和IEPAD。

2010年Song等人提出一种抽取用户生成数据（UGC）记录的方法MiBAT\citeup{song2010automatic}。
论坛帖子、评论回复等用户生成数据格式自由，可能包含图片和格式化标签，
前述方法主要针对高度格式化的数据，例如商品展示信息，从而忽略了UGC的影响。
MiBAT利用领域知识限制，即UGC中普遍存在的发布时间信息作为锚节点，来定位候选子树，
降低了UGC标签的影响，取得了较好的效果。

\section{研究内容与组织结构}

本文的主要研究内容是面向多通道爬虫系统的Web信息抽取技术，具体分为以下几个方面：

（1）针对新闻、博客这类正文集中的网站，提出了一种基于有效字符的Web内容抽取方法
CEVC（Content Extraction via Valid Characters）。
该方法主要基于这样的观察，网页中不属于链接并包含停止词的文本，更有可能是主要内容。
定义这样的字符为有效字符，根据它们在DOM树中的分布，逐级确定正文区域，并最终提取正文。
在知名的中文新闻、博客网站上采集网页进行实验，
与之前的算法CETR（基于文本标签比）、CETD（基于文本密度）
和CEPR（基于文本标签路径比）进行了比较。
实验结果表明，CEVC算法在各项评价指标上都优于CETR和CEPR，
虽然抽取性能和CETD相当，但在预处理阶段依赖更小，适用性更强。

（2）针对论坛网站，提出了一种论坛帖子抽取算法PEAN（Post Extraction via Anchor Nodes）。
该方法利用论坛帖子中普遍存在的发帖时间信息作为锚节点，
根据它们在DOM树中的分布情况，定位论坛帖子集中的区域，并结合树匹配算法，
在候选子树中过滤噪声，最终抽取出论坛帖子。
为了和同样利用发帖时间信息的帖子抽取算法MiBAT比较效果，我们从知名的中文论坛网站上采集网页
进行实验。
实验结果表明，PEAN相比于MiBAT在召回率指标上有大幅度提升，
总体F\textsubscript{1}指标也优于MiBAT。

（3）为了验证本文提出的信息抽取算法的实际效果，
根据实际需求设计并实现了一个Web新闻采集系统。
介绍了系统架构和总体设计方案，并对系统模块和关键技术做了详细阐述，
最后对系统运行效果进行评估。
由于使用了模板无关的信息抽取算法，该爬虫能够在较少人工辅助的情况下爬取新的网站，
大大减少了系统扩展和维护的成本。

本文组织结构如下：

第一章为绪论，
介绍了当前互联网舆情的发展形势，说明了自动化Web信息抽取技术在多通道爬虫系统中的关键作用，
并从Web内容抽取和Web数据记录抽取两方面回顾了国内外研究现状，
最后给出论文的研究内容与整体结构安排。

第二章为基于有效字符的Web内容抽取，
详述针对新闻、博客网站的Web内容抽取方法CEVC，并与CETR、CETD和CEPR进行了实验比较。

第三章为基于锚节点的论坛帖子抽取，
详述了针对论坛网站的帖子抽取方法PEAN，并与MiBAT进行了实验比较。

第四章为Web新闻采集系统的设计与实现，
详述了系统的总体设计方案、各模块的设计与实现，并对系统运行效果进行评估。
