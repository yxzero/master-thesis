%!TEX root = ../main.tex

\chapter{绪论}

\section{课题背景与研究意义}
随着Internet的迅猛发展，互联网应用已经深入到我国的经济、社会、文化、教育以及娱乐等各个方面，
成为人们生活中不可或缺的组成部分。
由于互联网平台具有自由便捷的发布和获取方式、快速的信息传播能力、极为广泛的地理覆盖程度，
Web正逐渐成为信息发布和消费的巨大平台，被称为独立于传统的报纸、广播和电视的“第四媒体”，
改变着信息在每个人中流动的方式。

在信息化技术高速发展的时代，网民人数不断增多，互联网信息量呈现指数型增长，
成为传达社情民意的重要渠道。
大众可以在互联网上自由、匿名地发表个人对社会现象、时事政治和热点事件的观点和态度，
再由网络迅速传播，不同意见随之碰撞、发酵形成舆情，进而影响社会舆论。
网络舆情可能包含危害国家安全和社会稳定的信息，为了维持社会稳定、减小一些舆情信息的负面影响，
关注互联网舆情，及时掌握社会舆论动态和当前热点话题，对维护社会安定非常重要。

如今网络舆情的表达方式多种多样，新闻、博客和论坛是其中重要的信息传播渠道。
阅读在线新闻是网民获取信息极为便捷的方式，
而博客和论坛都为普通大众提供了发表意见、分享观点的平台，因而形成了一个庞大的网络社区。
在一个聚焦于新闻、博客和论坛的多通道爬虫系统中，从下载的网页中抽取有用的信息，
供后续处理和分析，是一个非常重要的环节。
只抽取有用的信息而不是存储所有网页内容，能够节约存储和索引的空间，
提升自然语言处理和文本挖掘的效果。

和传统媒体不同，互联网提供的信息爆炸式增长，如何从海量信息中找到人们所需要的信息，
Web信息抽取发挥着关键的作用。
Web信息抽取是以Web作为信息来源的信息抽取方式，从网页的无结构或半结构信息中抽取用户感兴趣的内容，
转化为易于阅读和理解的格式，是信息能够被进一步分析和处理的基础。
由于网页所固有的半结构化和大量存在的与主题无关的噪声，从中有效地抽取信息并不是一个简单的工作。
针对一个多通道爬虫系统，我们还面临这些挑战：
\begin{itemize}
\item 大量网站需要关注
\item 网站具有不同的页面组织结构和布局
\item 网站会不定期升级改版
\end{itemize}
海量异构、持续变化的特点，给大范围舆情监控带来困难，迫切需要一种高度自动化的Web信息抽取方式，
尽可能减少人工参与，以降低系统扩展和维护的成本。

综上所述，为了适应互联网舆情监控的需求，需要对多通道爬虫系统中的Web信息抽取技术做进一步研究。
本文针对新闻、博客和论坛，研究了适应各自特点的Web信息抽取方式，在前人的基础上提出了改进的算法，
并通过详细的实验比较验证了不同方法的效果。
最后根据实际需求设计并实现了一个针对新闻的应用实例——Web新闻聚合系统，
以实际系统的运行效果，验证了本文提出的信息抽取技术的实际意义。

\section{国内外研究现状}
信息抽取（Information Extraction）最早可追溯到20世纪60年代中期，
作为自然语言处理的一个分支，研究如何从自然语言中获取结构化信息。
随着Web信息的爆炸式增长，Web正成为最大的信息平台，Web信息抽取逐渐成为新的研究热点。
与传统的文本信息抽取不同，Web信息抽取面对的是海量异构的语料，虽然以HTML标签作为分隔标志，
但整体缺乏严格统一的语法和语义信息，传统的文本信息抽取技术不能直接利用。
针对本文的研究内容，接下来主要从可用于新闻、博客的内容抽取技术，
和可用于论坛的数据记录抽取技术两方面，分析国内外研究现状。

\subsection{Web内容抽取}
网页内容抽取（Content Extraction）最早由Rahman等人于2001年提出，
并给出了一个基本的内容抽取算法\citeup{rahman2001content}。
以新闻网页为例，典型的Web网页除了主要内容之外，还包含大量与主题内容无关的噪声信息，
如导航栏、推荐链接、各种形式的广告等，如何从原始的网页中过滤噪声、抽取有用的信息，
是Web内容抽取的研究方向。

\subsubsection{基于包装器的内容抽取算法}

最简单和直接的Web内容抽取方法是手工构建包装器（Wrapper），
包装器是信息集成系统的一个独立模块，抽取网页数据并将其转换为结构化的数据。
但这个过程容易出错也很耗费人力资源，随后一些研究致力于简化包装器构造的过程，降低专业知识门槛，
例如XWrap\citeup{liu2000xwrap}和W4F\citeup{sahuguet2001building}。

XWrap（XML-enabled Wrapper）
通过和用户的交互，推导出信息抽取规则，交互式生成XML形式的包装器。
W4F（World Wide Web Wrapper Factory）
提供了一个更有表现力的领域专用语言描述复杂的抽取规则，并利用图形化工具辅助构造包装器。
这些包装器使用的技术包括正则表达式、XPath、甚至特定领域的编程语言，
用来抽取嵌入在半结构化HTML网页中的文本。

虽然这些手工构造的包装器针对特定站点能够取得很高的抽取准确率，但它们的缺点也很明显，
需要大量的人力成本，对大规模的内容抽取任务来说手工构造包装器是难以接受的。

\subsubsection{基于机器学习的内容抽取算法}

基于机器学习的方法主要利用网页的结构、语言学等特征，在人工标注的数据集上进行训练，
根据训练出的分类模型，区分网页中的主要内容和噪声数据。

Kushmerick\citeup{kushmerick1999learning}和Davison\citeup{davison2000recognizing}
提出使用机器学习的方法来识别网页中的广告、冗余和不相关的链接。
Marek等人\citeup{marek2007web}提出了一种利用
条件随机场（Conditional Random Fields，CRF）的方法来识别网页中的核心内容，
首先通过HTML标签，将训练文件分成若干块，对于每一块提取markup-based、content-based和
document-related的特征。依据这些特征，将内容块标记为若干类，训练条件随机场模型进行分类。
FIASCO\citeup{bauer2007fiasco}基于
支持向量机（Support Vector Machine，SVM）来识别网页中的噪声数据。
首先将网页文件解析成DOM树，对每一个目标节点提取语言学特征、结构特征和视觉特征，
人工选择目标节点标注为clean或dirty，然后使用SVM训练分类器。

然而这些方法难以推广使用，因为它们依赖大量人工标记的训练数据集和领域专业知识来生成分类规则。

\subsubsection{基于视觉信息的内容抽取算法}

2003年微软亚洲研究院的蔡登提出了一种基于视觉信息的抽取算法
VIPS（VIsion based Page Segmentation）\citeup{cai2003vips}。
基于前人对于用户视觉心理的研究，一般情况下网页的核心内容处于网页的中间位置，
而导航栏、广告和推广链接则处于网页的周边位置。
这种方法利用网页的视觉信息，根据一些启发式规则，首先将网页分块，对不同的分块赋予相应的权重，
然后进行删除、合并操作，最终确定网页的核心内容块。

随后宋瑞华\citeup{song2004learning}在VIPS的基础上，使用人工标注的训练集，
根据不同分块的位置特征和内容特征，用SVM和神经网络训练分类模型。
Fernandes\citeup{fernandes2007computing}首先使用VIPS网页分块算法，
将每个网页分成若干个由DOM树中的路径及其文本内容定义的块，
在上述基础上使用向量空间模型（Vector Space Model，VSM）思想，计算其重要程度。

基于视觉信息分块的方法具有很好的通用性，因为不同网页的源代码结构可能差别很大，
但经浏览器渲染之后的视觉表现却很接近。但这种方法必须对网页进行渲染才能利用视觉信息，
非常消耗计算资源，而且有时候渲染所需的网页样式文件不一定是可获取的。

\subsubsection{基于模板检测的内容抽取算法}

如今大部分网页都是使用固定模板动态生成的，网站后台程序从数据库中提取信息，
与模板进行渲染就形成了最终的HMTL源文件。
这样的模板在同一个网站被大量重复使用，就可以通过一组相同或相似模板生成的网页，
逆向推导出共同的模板结构，以此提取网页核心内容，这就是模板检测算法。

Lin等人\citeup{lin2002discovering}提出一种基于当时HTML文档普遍存在的制表标签的方法。
将网页划分为若干内容块，然后根据某些特征计算每个块的信息熵，
由动态调整的阈值把内容块划分为核心内容块和噪声数据。
Yi等人\citeup{yi2003eliminating}
在DOM树的基础上提出一个新型树结构，网站风格树（Site Style Tree，SST）。
由于噪声块通常有相似的内容和表现的样式，如果相同内容和样式在多个网页中重复出现，
表明其信息熵较低，可能是冗余内容。
文章提出一种基于信息熵的度量方法来确定SST的哪部分代表噪声，哪部分代表主要内容。
将网页DOM树匹配到网站的SST的过程，就能够检测并消除噪声部分，从而提取主要内容。

Bar-Yossef等人\citeup{bar2002template}
提出利用经典的频繁项挖掘来检测网页的模板，类似的研究还有\cite{chen2006template}。

模板检测算法在模板生成后，实际提取网页内容的计算开销很小。
但这类方法只能针对一个模板生成的网页进行处理，对于不同模板生成的网页，都需要重新构造模板。
随着Web的发展，一个网站所使用的模板越来越丰富和多样，网站还会不定期改版，
之前生成的模板就会失效，这些都给模板维护工作带来巨大负担。

\subsubsection{基于统计规律的内容抽取算法}

还有一类基于统计规律和启发式规则的内容提取算法，不需要一组网页进行训练因而与模板无关，
而且自动化程度较高。

BTE（Body Text Extraction）\citeup{finn2001fact}
将HTML网页处理为一个由词语和标签构成的序列，从中找到一个连续的区段，
使得这个区段包含最多的词语，并且包含最少的标签。
一般网页的正文包含较少的标签和相对密集的文字，这样的区段就认为是核心内容。
FE（Feature Extractor）\citeup{debnath2005automatic}
和KFE（K-Feature Extractor）\citeup{debnath2005identifying}
将网页划分为若干块，分析特定的文本、图片和标签等特征，
从中找出出现次数较少，并且最符合期望特征的块。
LQF（Link Quota Filters）\citeup{mantratzis2005separating}
利用网页分块中的链接比重来识别导航栏或相似的噪声内容。
停止词和其它一些启发式规则也在一些工作中出现\citeup{gottron2008combining}。

2008年的CCB（Content Code Blurring）\citeup{gottron2008content}
将网页HTML代码视为一个向量，生成二元的Content Code Vector（CCV）。
网页源文件中的文本内容在向量中为1，HTML代码在向量中为0，这样就把网页转换为一个二元向量。
随后对向量进行处理，从中找出同质化格式的内容，即连续为1的部分。
2010年的CETR（Content Extraction via Tag Ratios）\citeup{weninger2010cetr}
对网页源文件一行一行处理，计算出每行的Tag Ratios，即一行中的实际字符数除以HTML标签数。
Tag Ratio越高就越可能是主要内容，作者最后将计算结果映射到二维空间，利用聚类算法进行处理。
这两种算法简单、高效不需要训练数据，也不需要解析DOM树或者渲染网页，
但按行处理的方式没有利用HTML的结构化信息，对网页代码风格也较为敏感。

2011年的CETD（Content Extraction via Text Density）\citeup{sun2011dom}
首先将HTML网页解析为DOM树，然后对每个节点计算Text Density，
即该DOM节点包含的字符数除以标签数，在考虑到链接与噪声的相关性后，
提出Composite Text Density，对文本密度做了一些修正。
最后提出一种结合DOM树的DensitySum计算方式替换常用的平滑和阈值划分方法，最终确定核心内容块。

2013年的CEPR（Content Extraction via Path Ratios）\citeup{wu2013web}
认为网页内容布局与其DOM树的标签路径之间存在隐含的关联，针对每个节点计算文本标签路径比，
即文本长度与标签路径出现次数的比值。
文本标签路径比越高，表明这个路径模式承载的文本信息越多，越可能是主要内容。
在\cite{weninger2010cetr}的高斯平滑算法基础上，
作者提出以编辑距离作为权重，改善平滑效果，然后通过阈值方法抽取主要内容。

\subsection{Web数据记录抽取}
网页具有半结构化的特点，不同于XML或数据库记录等高度结构化的文档，不能被计算机程序直接读取。
Web数据记录抽取是从网页中抽取结构化数据，例如抽取、整合来自不同数据源的商品的各项元数据信息，
抽取论坛中帖子的发帖时间、发帖内容等元数据信息。

\subsubsection{半自动数据记录抽取算法}

人工编写包装器来提取网页信息的方法需要大量人力投入，成本高也并不实用。
于是研究人员实现了一系列工具来自动生成包装器，能够从用户的训练样本中归纳学习抽取规则，
降低了包装器构造的成本。
WIEN\citeup{kushmerick1997wrapper}，SoftMealy\citeup{hsu1998generating}
和Stalker\citeup{muslea1999hierarchical}是这类半自动化方法的典型代表。

WIEN基于多种不同的归纳学习技术，并提出了一个混合系统，训练阶段只需要少量人工参与。
SoftMealy基于有限状态转换器（Finite State Transducer，FST）和上下文规则，
通过自底向上的归纳学习方法，由人工标注的样本文件生成包装器。
它们的主要区别是抽取架构不同，WIEN使用一遍处理的LR结构，Stalker使用多遍处理的层级结构。

国内孟小峰\citeup{孟小峰2001xwis}
提出了一种基于预定义模式的方法来构造HTML包装器，
并将它运用到XWIS（基于XML的Web信息查询系统）中，
由用户定义模式并给出模式与HTML页面的映射关系，随后推导出规则构造包装器。

另一类基于模板树匹配的方法，首先从标注数据生成模板树，
模板树中映射了数据记录的相应位置，
随后对同一类型的网页，运行树匹配算法，即可抽取结构化数据。
Chuang等人\citeup{chuang2004tree}
提出一种模板树自动生成算法TTAG，能够从少量的训练网页中学习模板的树型结构，
适用于频繁更新的网站。
Zheng等人\citeup{zheng2009efficient}
提出一种新的Broom结构来同时表示数据记录和生成的模板，能够有效抽取数据记录并识别内部的语义。

国内李效东\citeup{李效东2002基于}
提出一种归纳学习算法来半自动地生成提取规则，
利用DOM树中的路径作为信息抽取的坐标，指导抽取过程。
算法是一个典型的顺序覆盖算法，产生一个假设去覆盖集合中尽可能多的正例，再删除被覆盖的正例，
循环直至所有的元素被覆盖。

这类半自动化方法需要人工标注的样本，这个过程依然费时费力，
另外模板的生成和维护都需要很大的成本，不适用于互联网规模的网络信息抽取工作。

\subsubsection{全自动数据记录抽取算法}

为了克服依赖人工标注数据的缺陷，一部分研究工作聚焦于全自动化的信息抽取方法，
这类方法更适合互联网环境下的大规模信息抽取工作。

Embley\citeup{embley1999record}
提出一种基于启发式规则的数据记录抽取方法，将网页文档的结构以嵌套标签树的形式刻画，
然后定位出包含数据记录的子树，使用五种独立的启发式规则来识别候选的记录分隔符。
Buttler提出一个全自动化的对象抽取系统Omini\citeup{buttler2001fully}，
Omini将网页解析为树型结构然后分两步抽取对象，定位子树和确定记录分隔符。
它同样采用了多种启发式规则，主要贡献是自动化的规则学习算法。
这部分方法过于依赖启发式规则，难以大规模扩展，而且如今的网页结构和布局也发生了很大变化。

Chang等人提出一个能够从网页中自动发现抽取规则的系统IEPAD\citeup{chang2001iepad}，
识别数据记录用到了重复模式的挖掘和多序列对齐（multiple sequence alignment）。
重复模式的挖掘通过一种新的数据结构PAT树来实现，并经过模式对齐进一步扩展以理解所有的记录实例。
Wang等人在此基础上提出了DeLa\citeup{wang2003data}，
该系统通过HTML表单提交查询，自动生成基于正则表达式的包装器从结果页中抽取数据对象。

上述单纯基于HTML标签序列模式特征的方法不能很好地利用网页的层次结构信息，
部分研究人员开始在DOM树上识别重复相似子树，进而抽取数据记录，
MDR\citeup{liu2003mining}是其中的典型代表，它依据这样的假设：
1）一组相似的数据记录通常位于连续的区域并具有相似的HTML标签结构；
2）每个数据记录之下包含相同数目的兄弟子树。
MDR方法简单，抽取结果也由于Omini和IEPAD。

2010年Song等人提出一种抽取用户生成数据（UGC）记录的方法MiBAT\citeup{song2010automatic}。
论坛帖子、评论回复等用户生成数据格式自由，可能包含图片和格式化标签，
前述方法主要针对高度格式化的数据，例如商品展示信息，从而忽略了UGC的影响。
MiBAT利用领域知识限制，即UGC中普遍存在的发布时间信息作为锚节点，来定位候选子树，
降低了UGC标签的影响，取得了较好的效果。

\section{研究内容与组织结构}

本文的主要研究内容是面向多通道爬虫系统的Web信息抽取技术，具体分为以下几个方面：

（1）针对新闻、博客这类正文集中的网站，提出了一种基于有效字符的Web内容抽取方法
CEVC（Content Extraction via Valid Characters）。
该方法主要基于这样的观察，网页中不属于链接并包含停止词的文本，更有可能是主要内容。
定义这样的字符为有效字符，根据它们在DOM树中的分布，逐级确定正文区域，并最终提取正文。
在知名的中文新闻、博客网站上采集网页进行实验，
与之前的算法CETR（基于文本标签比）、CETD（基于文本密度）
和CEPR（基于文本标签路径比）进行了比较。
实验结果表明，CEVC算法在各项评价指标上都优于CETR和CEPR，
虽然抽取性能和CETD相当，但在预处理阶段依赖更小，适用性更强。

（2）针对论坛网站，提出了一种论坛帖子抽取算法PEAN（Post Extraction via Anchor Nodes）。
该方法利用论坛帖子中普遍存在的发帖时间信息作为锚节点，
根据它们在DOM树中的分布情况，定位论坛帖子集中的区域，并结合树匹配算法，
在候选子树中过滤噪声，最终抽取出论坛帖子。
为了和同样利用发帖时间信息的帖子抽取算法MiBAT比较效果，我们从知名的中文论坛网站上采集网页
进行实验。
实验结果表明，PEAN相比于MiBAT在召回率指标上有大幅度提升，
总体F\textsubscript{1}指标也优于MiBAT。

（3）为了验证本文提出的信息抽取算法的实际效果，
根据实际需求设计并实现了一个Web新闻采集系统。
介绍了系统架构和总体设计方案，并对系统模块和关键技术做了详细阐述，
最后对系统运行效果进行评估。
由于使用了模板无关的信息抽取算法，该爬虫能够在较少人工辅助的情况下爬取新的网站，
大大减少了系统扩展和维护的成本。

本文组织结构如下：

第一章为\textbf{绪论}，
介绍了当前互联网舆情的发展形势，说明了自动化Web信息抽取技术在多通道爬虫系统中的关键作用，
并从Web内容抽取和Web数据记录抽取两方面回顾了国内外研究现状，
最后给出论文的研究内容与整体结构安排。

第二章为\textbf{基于有效字符的Web内容抽取}，
详述针对新闻、博客网站的Web内容抽取方法CEVC，并与CETR、CETD和CEPR进行了实验比较。

第三章为\textbf{基于锚节点的论坛帖子抽取}，
详述了针对论坛网站的帖子抽取方法PEAN，并与MiBAT进行了实验比较。

第四章为\textbf{Web新闻采集系统的设计与实现}，
详述了系统的总体设计方案、各模块的设计与实现，并对系统运行效果进行评估。
